{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 13:38:05.460095: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-24 13:38:05.913912: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-24 13:38:07.734283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib:/usr/local/cuda-11.2/lib64/:/usr/local/cuda-11.2/lib64/\n",
      "2025-01-24 13:38:07.734850: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib:/usr/local/cuda-11.2/lib64/:/usr/local/cuda-11.2/lib64/\n",
      "2025-01-24 13:38:07.734870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#all imports\n",
    "import os\n",
    "import carla #the sim library itself\n",
    "import time # to set a delay after each photo\n",
    "import cv2 #to work with images from cameras\n",
    "import numpy as np #in this example to change image representation - re-shaping\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "sys.path.append('/opt/carla-simulator/PythonAPI/carla') # tweak to where you put carla\n",
    "from keras.models import load_model\n",
    "from agents.navigation.global_route_planner import GlobalRoutePlanner\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "# disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('10.8.179.139', 2000)\n",
    "# start a car\n",
    "mapid = 4\n",
    "client.load_world('Town0' + str(mapid))\n",
    "world = client.get_world()\n",
    "\n",
    "#clean up\n",
    "for actor in world.get_actors().filter('*vehicle*'):\n",
    "    actor.destroy()\n",
    "for sensor in world.get_actors().filter('*sensor*'):\n",
    "    sensor.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic settings\n",
    "PREFERRED_SPEED = 30\n",
    "SPEED_THRESHOLD = 2 # defines when we get close to desired speed so we drop the speed\n",
    "\n",
    "# Max steering angle\n",
    "MAX_STEER_DEGREES = 40\n",
    "# This is max actual angle with Mini under steering input=1.0\n",
    "STEERING_CONVERSION = 75\n",
    "\n",
    "CAMERA_POS_Z = 1.3 \n",
    "CAMERA_POS_X = 1.4 \n",
    "\n",
    "# resize images before running thgem through the model\n",
    "# this is the same as when yo train the model\n",
    "HEIGHT = 66\n",
    "WIDTH = 200\n",
    "\n",
    "#adding params to display text to image\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# org - defining lines to display telemetry values on the screen\n",
    "org = (30, 30) # this line will be used to show current speed\n",
    "org2 = (30, 50) # this line will be used for future steering angle\n",
    "org3 = (30, 70) # and another line for future telemetry outputs\n",
    "org4 = (30, 90) # and another line for future telemetry outputs\n",
    "org3 = (30, 110) # and another line for future telemetry outputs\n",
    "fontScale = 0.5\n",
    "# white color\n",
    "color = (255, 255, 255)\n",
    "# Line thickness of 2 px\n",
    "thickness = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 13:38:35.050188: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-01-24 13:38:35.050337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (HASS-DESKTOP6): /proc/driver/nvidia/version does not exist\n",
      "2025-01-24 13:38:35.052086: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# utility function for camera listening \n",
    "def camera_callback(image,data_dict):\n",
    "    data_dict['image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))[:, :, :3]\n",
    "\n",
    "# utility function for camera listening \n",
    "def sem_callback(image,data_dict):\n",
    "    ########## IMPORTANT CHANGE for Semantic camera ##############\n",
    "    image.convert(carla.ColorConverter.CityScapesPalette)\n",
    "    data_dict['sem_image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))[:, :, :3]\n",
    "\n",
    "# maintain speed function\n",
    "def maintain_speed(s):\n",
    "    ''' \n",
    "    this is a very simple function to maintan desired speed\n",
    "    s arg is actual current speed\n",
    "    '''\n",
    "    if s >= PREFERRED_SPEED:\n",
    "        return 0\n",
    "    elif s < PREFERRED_SPEED - SPEED_THRESHOLD:\n",
    "        return 0.9 # think of it as % of \"full gas\"\n",
    "    else:\n",
    "        return 0.4 # tweak this if the car is way over or under preferred speed \n",
    "\n",
    "\n",
    "# function to get angle between the car and target waypoint\n",
    "def get_angle(car,wp):\n",
    "    '''\n",
    "    this function returns degrees between the car's direction \n",
    "    and direction to a selected waypoint\n",
    "    '''\n",
    "    vehicle_pos = car.get_transform()\n",
    "    car_x = vehicle_pos.location.x\n",
    "    car_y = vehicle_pos.location.y\n",
    "    wp_x = wp.transform.location.x\n",
    "    wp_y = wp.transform.location.y\n",
    "    \n",
    "    # vector to waypoint\n",
    "    x = (wp_x - car_x)/((wp_y - car_y)**2 + (wp_x - car_x)**2)**0.5\n",
    "    y = (wp_y - car_y)/((wp_y - car_y)**2 + (wp_x - car_x)**2)**0.5\n",
    "    \n",
    "    #car vector\n",
    "    car_vector = vehicle_pos.get_forward_vector()\n",
    "    degrees = math.degrees(np.arctan2(y, x) - np.arctan2(car_vector.y, car_vector.x))\n",
    "    # extra checks on predicted angle when values close to 360 degrees are returned\n",
    "    if degrees<-180:\n",
    "        degrees = degrees + 360\n",
    "    elif degrees > 180:\n",
    "        degrees = degrees - 360\n",
    "    return degrees\n",
    "\n",
    "def get_proper_angle(car,wp_idx,rte):\n",
    "    '''\n",
    "    This function uses simple fuction above to get angle but for current\n",
    "    waypoint and a few more next waypoints to ensure we have not skipped\n",
    "    next waypoint so we avoid the car trying to turn back\n",
    "    '''\n",
    "    # create a list of angles to next 5 waypoints starting with current\n",
    "    next_angle_list = []\n",
    "    for i in range(10):\n",
    "        if wp_idx + i*3 <len(rte)-1:\n",
    "            next_angle_list.append(get_angle(car,rte[wp_idx + i*3][0]))\n",
    "    idx = 0\n",
    "    while idx<len(next_angle_list)-2 and abs(next_angle_list[idx])>40:\n",
    "        idx +=1\n",
    "    return wp_idx+idx*3,next_angle_list[idx]  \n",
    "\n",
    "\n",
    "def draw_route(wp, route,seconds=3.0):\n",
    "    #draw the next few points route in sim window - Note it does not\n",
    "    # get into the camera of the car\n",
    "    if len(route)-wp <25: # route within 25 points from end is red\n",
    "        draw_colour = carla.Color(r=255, g=0, b=0)\n",
    "    else:\n",
    "        draw_colour = carla.Color(r=0, g=0, b=255)\n",
    "    for i in range(10):\n",
    "        if wp+i<len(route)-2:\n",
    "            world.debug.draw_string(route[wp+i][0].transform.location, '^', draw_shadow=False,\n",
    "                color=draw_colour, life_time=seconds,\n",
    "                persistent_lines=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "def select_random_route(position,locs):\n",
    "    '''\n",
    "    retruns a random route for the car/veh\n",
    "    out of the list of possible locations locs\n",
    "    where distance is longer than 100 waypoints\n",
    "    '''    \n",
    "    point_a = position.location #we start at where the car is or last waypoint\n",
    "    sampling_resolution = 1\n",
    "    grp = GlobalRoutePlanner(world.get_map(), sampling_resolution)\n",
    "    # now let' pick the longest possible route\n",
    "    min_distance = 100\n",
    "    result_route = None\n",
    "    route_list = []\n",
    "    for loc in locs: # we start trying all spawn points \n",
    "                                #but we just exclude first at zero index\n",
    "        cur_route = grp.trace_route(point_a, loc.location)\n",
    "        if len(cur_route) > min_distance:\n",
    "            route_list.append(cur_route)\n",
    "    result_route = random.choice(route_list)\n",
    "    return result_route\n",
    "\n",
    "def exit_clean():\n",
    "    #clean up\n",
    "    cv2.destroyAllWindows()\n",
    "    for sensor in world.get_actors().filter('*sensor*'):\n",
    "        sensor.destroy()\n",
    "    for actor in world.get_actors().filter('*vehicle*'):\n",
    "        actor.destroy()\n",
    "    return None\n",
    "\n",
    "def predict_angle(RGB_im):\n",
    "    img = np.float32(RGB_im)\n",
    "    img = img /255              # normalize\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    angle = model(img,training=False)\n",
    "    return angle.numpy()[0][0] / math.pi\n",
    "\n",
    "\n",
    "# spawn the car\n",
    "world = client.get_world()\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "#look for a blueprint of Tesla m3 car\n",
    "vehicle_bp = world.get_blueprint_library().filter('*model3*')\n",
    "\n",
    "# load Nvidia DAVE-2 model\n",
    "MODEL_NAME = 'model/model.h5'\n",
    "model = load_model(MODEL_NAME,compile=False)\n",
    "model.compile()\n",
    "quit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predict: 76.835 \t Actual Angle: 0.001969\n",
      "Model predict: 75.936 \t Actual Angle: 0.001969\n",
      "Model predict: 76.603 \t Actual Angle: 0.001969\n",
      "Model predict: 75.474 \t Actual Angle: 0.001979\n",
      "Model predict: 75.088 \t Actual Angle: 0.002033\n",
      "Model predict: 76.069 \t Actual Angle: 0.002033\n",
      "Model predict: 76.027 \t Actual Angle: 0.002033\n",
      "Model predict: 75.525 \t Actual Angle: 0.002033\n",
      "Model predict: 77.310 \t Actual Angle: 0.002033\n",
      "Model predict: 76.645 \t Actual Angle: 0.002033\n",
      "Model predict: 76.426 \t Actual Angle: 0.002033\n",
      "Model predict: 77.620 \t Actual Angle: 0.002033\n",
      "Model predict: 76.439 \t Actual Angle: 0.002033\n",
      "Model predict: 77.614 \t Actual Angle: 0.002061\n",
      "Model predict: 76.731 \t Actual Angle: 0.002061\n",
      "Model predict: 76.630 \t Actual Angle: 0.002061\n",
      "Model predict: 76.862 \t Actual Angle: 0.002061\n",
      "Model predict: 76.949 \t Actual Angle: 0.002061\n",
      "Model predict: 76.887 \t Actual Angle: 0.002061\n",
      "Model predict: 77.716 \t Actual Angle: 0.002061\n",
      "Model predict: 76.475 \t Actual Angle: 0.002061\n",
      "Model predict: 75.390 \t Actual Angle: 0.002061\n",
      "Model predict: 75.501 \t Actual Angle: 0.002061\n",
      "Model predict: 76.850 \t Actual Angle: 0.002061\n",
      "Model predict: 76.987 \t Actual Angle: 0.002061\n",
      "Model predict: 76.274 \t Actual Angle: 0.002184\n",
      "Model predict: 75.912 \t Actual Angle: 0.002184\n",
      "Model predict: 77.486 \t Actual Angle: 0.002184\n",
      "Model predict: 76.405 \t Actual Angle: 0.002184\n",
      "Model predict: 75.920 \t Actual Angle: 0.002184\n",
      "Model predict: 76.200 \t Actual Angle: 0.002184\n",
      "Model predict: 75.269 \t Actual Angle: 0.002184\n",
      "Model predict: 75.702 \t Actual Angle: 0.002184\n",
      "Model predict: 75.549 \t Actual Angle: 0.002174\n",
      "Model predict: 77.145 \t Actual Angle: 0.002184\n",
      "Model predict: 75.567 \t Actual Angle: 0.002174\n",
      "Model predict: 76.340 \t Actual Angle: 0.002174\n",
      "Model predict: 76.568 \t Actual Angle: 0.002174\n",
      "Model predict: 76.221 \t Actual Angle: 0.002174\n",
      "Model predict: 76.094 \t Actual Angle: 0.002154\n",
      "Model predict: 76.782 \t Actual Angle: -0.211674\n",
      "Model predict: 81.870 \t Actual Angle: -0.680856\n",
      "Model predict: 93.133 \t Actual Angle: -1.313450\n",
      "Model predict: 98.761 \t Actual Angle: -2.082482\n",
      "Model predict: 109.839 \t Actual Angle: -2.948257\n",
      "Model predict: 101.588 \t Actual Angle: -4.006491\n",
      "Model predict: 106.059 \t Actual Angle: -5.104549\n",
      "Model predict: 111.016 \t Actual Angle: -6.538710\n",
      "Model predict: 113.377 \t Actual Angle: -7.925631\n",
      "Model predict: 123.050 \t Actual Angle: -9.702241\n",
      "Model predict: 106.332 \t Actual Angle: -11.321091\n",
      "Model predict: 136.265 \t Actual Angle: -13.247210\n",
      "Model predict: 151.463 \t Actual Angle: -14.272269\n",
      "Model predict: 143.754 \t Actual Angle: -15.766071\n",
      "Model predict: 165.487 \t Actual Angle: -17.350046\n",
      "Model predict: 163.361 \t Actual Angle: -19.253382\n",
      "Model predict: 126.758 \t Actual Angle: -21.231306\n",
      "Model predict: 131.423 \t Actual Angle: -24.200358\n",
      "Model predict: 146.324 \t Actual Angle: -25.635092\n",
      "Model predict: 121.495 \t Actual Angle: -28.219434\n",
      "Model predict: 104.363 \t Actual Angle: -32.276869\n",
      "Model predict: 71.673 \t Actual Angle: -34.382953\n",
      "Model predict: 72.099 \t Actual Angle: -35.687112\n",
      "Model predict: 80.503 \t Actual Angle: -35.711582\n",
      "Model predict: 84.162 \t Actual Angle: -37.309397\n",
      "Model predict: 89.344 \t Actual Angle: -38.906297\n",
      "Model predict: 77.121 \t Actual Angle: -36.644269\n",
      "Model predict: 75.151 \t Actual Angle: -38.154893\n",
      "Model predict: 91.430 \t Actual Angle: -39.622307\n",
      "Model predict: 89.888 \t Actual Angle: -38.732611\n",
      "Model predict: 83.564 \t Actual Angle: -38.511331\n",
      "Model predict: 83.984 \t Actual Angle: -39.866972\n",
      "Model predict: 81.058 \t Actual Angle: -42.015039\n",
      "Model predict: 46.087 \t Actual Angle: -41.058109\n",
      "Model predict: 45.944 \t Actual Angle: -41.210678\n",
      "Model predict: 59.621 \t Actual Angle: -41.298042\n",
      "Model predict: 51.095 \t Actual Angle: -41.681704\n",
      "Model predict: 44.237 \t Actual Angle: -42.247948\n",
      "Model predict: 41.475 \t Actual Angle: -42.444718\n",
      "Model predict: 54.678 \t Actual Angle: -42.869979\n",
      "Model predict: 76.912 \t Actual Angle: -43.287990\n",
      "Model predict: 89.388 \t Actual Angle: -43.792065\n",
      "Model predict: 93.716 \t Actual Angle: -44.524670\n",
      "Model predict: 85.378 \t Actual Angle: -45.431496\n",
      "Model predict: 88.111 \t Actual Angle: -46.702737\n",
      "Model predict: 85.685 \t Actual Angle: -49.626629\n",
      "Model predict: 57.555 \t Actual Angle: -53.164335\n",
      "Model predict: 43.483 \t Actual Angle: -57.140544\n",
      "Model predict: 36.595 \t Actual Angle: -61.382721\n",
      "Model predict: -0.400 \t Actual Angle: -65.439393\n",
      "Model predict: -1.003 \t Actual Angle: -69.187295\n",
      "Model predict: -1.298 \t Actual Angle: -72.632828\n",
      "Model predict: -1.593 \t Actual Angle: -75.595775\n",
      "Model predict: -1.700 \t Actual Angle: -78.416176\n",
      "Model predict: -1.740 \t Actual Angle: -81.103756\n",
      "Model predict: -1.771 \t Actual Angle: -83.662428\n",
      "Model predict: -1.822 \t Actual Angle: -86.359349\n",
      "Model predict: -1.702 \t Actual Angle: -89.095452\n",
      "Model predict: -1.570 \t Actual Angle: -91.861396\n",
      "Model predict: -1.514 \t Actual Angle: -94.654246\n",
      "Model predict: -1.501 \t Actual Angle: -97.466646\n",
      "Model predict: -1.571 \t Actual Angle: -100.286114\n",
      "Model predict: -1.634 \t Actual Angle: -103.102657\n",
      "Model predict: -1.828 \t Actual Angle: -105.907682\n",
      "Model predict: -1.932 \t Actual Angle: -108.690850\n",
      "Model predict: -1.809 \t Actual Angle: -111.443766\n",
      "Model predict: -1.146 \t Actual Angle: -114.155963\n",
      "Model predict: 0.022 \t Actual Angle: -116.809399\n",
      "Model predict: 0.479 \t Actual Angle: -119.558888\n",
      "Model predict: -0.801 \t Actual Angle: -122.438633\n",
      "Model predict: -0.824 \t Actual Angle: -125.445609\n",
      "Model predict: 0.306 \t Actual Angle: -128.521058\n",
      "Model predict: 0.906 \t Actual Angle: -131.709617\n",
      "Model predict: 1.148 \t Actual Angle: -134.940789\n",
      "Model predict: 1.340 \t Actual Angle: -138.172196\n",
      "Model predict: 1.341 \t Actual Angle: -141.397568\n",
      "Model predict: 1.457 \t Actual Angle: -144.614738\n",
      "Model predict: 1.595 \t Actual Angle: -147.811216\n",
      "Model predict: 1.801 \t Actual Angle: -150.989114\n",
      "Model predict: 1.902 \t Actual Angle: -154.060450\n",
      "Model predict: 1.907 \t Actual Angle: -157.179053\n",
      "Model predict: 1.897 \t Actual Angle: -160.257271\n",
      "Model predict: 1.949 \t Actual Angle: -164.391091\n",
      "Model predict: 2.068 \t Actual Angle: -169.250211\n",
      "Model predict: 2.127 \t Actual Angle: -172.335511\n",
      "Model predict: 2.436 \t Actual Angle: -175.402478\n",
      "Model predict: 3.773 \t Actual Angle: -179.031019\n",
      "Model predict: 3.826 \t Actual Angle: 176.636172\n",
      "Model predict: 4.510 \t Actual Angle: 171.495761\n",
      "Model predict: 5.723 \t Actual Angle: 166.044789\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "while True:\n",
    "    start_point = random.choice(spawn_points)\n",
    "    vehicle = world.try_spawn_actor(vehicle_bp[0], start_point)\n",
    "    time.sleep(2)\n",
    "    #setting RGB Camera - this follow the approach explained in a Carla video\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', '640') # this ratio works in CARLA 9.14 on Windows\n",
    "    camera_bp.set_attribute('image_size_y', '360')\n",
    "    camera_init_trans = carla.Transform(carla.Location(z=CAMERA_POS_Z,x=CAMERA_POS_X))\n",
    "    #this creates the camera in the sim\n",
    "    camera = world.spawn_actor(camera_bp,camera_init_trans,attach_to=vehicle)\n",
    "    image_w = camera_bp.get_attribute('image_size_x').as_int()\n",
    "    image_h = camera_bp.get_attribute('image_size_y').as_int()\n",
    "\n",
    "    camera_data = {'image': np.zeros((image_h,image_w,3))} # 3 channels for RGB\n",
    "\n",
    "    # this actually opens a live stream from the camera\n",
    "    camera.listen(lambda image: camera_callback(image,camera_data))\n",
    "    cv2.namedWindow('RGB Camera',cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('RGB Camera', 800, 450)\n",
    "    cv2.imshow('RGB Camera',camera_data['image'])\n",
    "    # getting a random route for the car\n",
    "    route = select_random_route(start_point,spawn_points)\n",
    "    curr_wp = 5 #we will be tracking waypoints in the route and switch to next one when we get close to current one\n",
    "    predicted_angle = 0\n",
    "    PREFERRED_SPEED = 20 # setting speed at start of new route\n",
    "    \n",
    "    spectator = world.get_spectator()\n",
    "    spectator_pos = carla.Transform(start_point.location + carla.Location(x=-20,y=10,z=10),\n",
    "                                carla.Rotation(yaw = start_point.rotation.yaw -155))\n",
    "    spectator.set_transform(spectator_pos)\n",
    "\n",
    "    while curr_wp<len(route)-1:\n",
    "        # Carla Tick\n",
    "        world.tick()\n",
    "        draw_route(curr_wp, route,1)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            quit = True\n",
    "            exit_clean()\n",
    "            cv2.imwrite('final_%3f_%3s.png' % (steer_input,round(predicted_angle,0)), image)\n",
    "            break\n",
    "\n",
    "        image = camera_data['image']\n",
    "        image_show = image\n",
    "        image = cv2.resize(image, (WIDTH,HEIGHT), interpolation=cv2.INTER_AREA) # to get better img quality\n",
    "\n",
    "\n",
    "        # Spectator Update\n",
    "        spectator_transform = vehicle.get_transform()\n",
    "        spectator_transform.location += carla.Location(x=0, y=0, z=15)\n",
    "        spectator_transform.rotation.yaw += -15  # left\n",
    "        spectator_transform.rotation.pitch = -60 # downward\n",
    "        spectator.set_transform(spectator_transform)\n",
    "        \n",
    "        \n",
    "        if curr_wp >=len(route)-10: # within 10 points of end, the route is done\n",
    "            PREFERRED_SPEED = 0 # seeting speed to 0 after completing one route\n",
    "            exit_clean()\n",
    "            break\n",
    "        while curr_wp<len(route)-2 and vehicle.get_transform().location.distance(route[curr_wp][0].transform.location)<5:\n",
    "            curr_wp +=1 #move to next wp if we are too close\n",
    "        curr_wp, predicted_angle = get_proper_angle(vehicle,curr_wp,route)\n",
    "        \n",
    "        v = vehicle.get_velocity()\n",
    "        speed = round(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2),0)\n",
    "\n",
    "        # estimated_throttle = maintain_speed(speed)\n",
    "        # use the model to predict steering - predictions are expected to be in -1 to +1\n",
    "        steer_input = predict_angle(image)\n",
    "        throttle = 0.3 - 0.09 * abs(steer_input)\n",
    "\n",
    "        \n",
    "        vehicle.apply_control(carla.VehicleControl(throttle=float(throttle), steer=float(steer_input)))\n",
    "\n",
    "        image_show = cv2.UMat(image_show)\n",
    "        image_show = cv2.putText(image_show, 'Speed: '+str(int(speed))+' kmh', org, \n",
    "                        font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        image_show = cv2.putText(image_show, 'Actual Angle: '+ str(int(predicted_angle)), org2, \n",
    "                        font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        image_show = cv2.putText(image_show, 'Model predict: '+str(float(steer_input)), org3, \n",
    "                        font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        print(\"Model predict: %.3f \\t Actual Angle: %f\" % (steer_input * 180, predicted_angle))\n",
    "        cv2.imshow('RGB Camera', cv2.resize(image_show, (800, 398)))\n",
    "    if quit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv2D)              (None, 31, 98, 24)        1824      \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 14, 47, 36)        21636     \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 5, 22, 48)         43248     \n",
      "                                                                 \n",
      " conv4 (Conv2D)              (None, 3, 20, 64)         27712     \n",
      "                                                                 \n",
      " conv5 (Conv2D)              (None, 1, 18, 64)         36928     \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 100)               115300    \n",
      "                                                                 \n",
      " do1 (Dropout)               (None, 100)               0         \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 50)                5050      \n",
      "                                                                 \n",
      " do2 (Dropout)               (None, 50)                0         \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 10)                510       \n",
      "                                                                 \n",
      " do3 (Dropout)               (None, 10)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,219\n",
      "Trainable params: 252,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name : conv1      layer output shape : (None, 31, 98, 24)   {'name': 'conv1', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 66, 200, 3), 'filters': 24, 'kernel_size': (5, 5), 'strides': (2, 2), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'elu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Layer name : conv2      layer output shape : (None, 14, 47, 36)   {'name': 'conv2', 'trainable': True, 'dtype': 'float32', 'filters': 36, 'kernel_size': (5, 5), 'strides': (2, 2), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'elu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Layer name : conv3      layer output shape : (None, 5, 22, 48)    {'name': 'conv3', 'trainable': True, 'dtype': 'float32', 'filters': 48, 'kernel_size': (5, 5), 'strides': (2, 2), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'elu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Layer name : conv4      layer output shape : (None, 3, 20, 64)    {'name': 'conv4', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'elu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Layer name : conv5      layer output shape : (None, 1, 18, 64)    {'name': 'conv5', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'elu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Layer name : flatten_5  layer output shape : (None, 1152)         {'name': 'flatten_5', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}\n",
      "Layer name : fc1        layer output shape : (None, 100)          {'name': 'fc1', 'trainable': True, 'dtype': 'float32', 'units': 100, 'activation': 'elu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Layer name : do1        layer output shape : (None, 100)          {'name': 'do1', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}\n",
      "Layer name : fc2        layer output shape : (None, 50)           {'name': 'fc2', 'trainable': True, 'dtype': 'float32', 'units': 50, 'activation': 'elu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Layer name : do2        layer output shape : (None, 50)           {'name': 'do2', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}\n",
      "Layer name : fc3        layer output shape : (None, 10)           {'name': 'fc3', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'elu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Layer name : do3        layer output shape : (None, 10)           {'name': 'do3', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}\n",
      "Layer name : dense_6    layer output shape : (None, 1)            {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(f'Layer name : {layer.name:<10} layer output shape : {str(layer.output_shape):<20} {layer.get_config()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name : conv1/kernel:0       weight shape : (5, 5, 3, 24)  \n",
      "Layer name : conv1/bias:0         weight shape : (24,)          \n",
      "Layer name : conv2/kernel:0       weight shape : (5, 5, 24, 36) \n",
      "Layer name : conv2/bias:0         weight shape : (36,)          \n",
      "Layer name : conv3/kernel:0       weight shape : (5, 5, 36, 48) \n",
      "Layer name : conv3/bias:0         weight shape : (48,)          \n",
      "Layer name : conv4/kernel:0       weight shape : (3, 3, 48, 64) \n",
      "Layer name : conv4/bias:0         weight shape : (64,)          \n",
      "Layer name : conv5/kernel:0       weight shape : (3, 3, 64, 64) \n",
      "Layer name : conv5/bias:0         weight shape : (64,)          \n",
      "Layer name : fc1/kernel:0         weight shape : (1152, 100)    \n",
      "Layer name : fc1/bias:0           weight shape : (100,)         \n",
      "Layer name : fc2/kernel:0         weight shape : (100, 50)      \n",
      "Layer name : fc2/bias:0           weight shape : (50,)          \n",
      "Layer name : fc3/kernel:0         weight shape : (50, 10)       \n",
      "Layer name : fc3/bias:0           weight shape : (10,)          \n",
      "Layer name : dense_6/kernel:0     weight shape : (10, 1)        \n",
      "Layer name : dense_6/bias:0       weight shape : (1,)           \n"
     ]
    }
   ],
   "source": [
    "for weight in model.weights:\n",
    "    print(f'Layer name : {weight.name:<20} weight shape : {str(weight.shape):<15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 104ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# 입력 이미지 로드 및 전처리\n",
    "image = cv2.imread('lane_detect.png')\n",
    "image = np.float32(image)\n",
    "image = image / 255\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# 특징 맵 추출\n",
    "layer_outputs = [layer.output for layer in model.layers[:5]]  # 처음 5개 레이어의 출력\n",
    "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(image)\n",
    "\n",
    "# 레이어 이름 설정\n",
    "layer_names = []\n",
    "for layer in model.layers[:5]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "# 특징 맵 시각화\n",
    "for layer_name, activation in zip(layer_names, activations):\n",
    "    n_features = activation.shape[-1]  # 특징 맵 개수 (채널 수)\n",
    "    height = activation.shape[1] # 특징 맵 높이\n",
    "    width = activation.shape[2]  # 특징 맵 너비\n",
    "\n",
    "    # 한 행에 표시할 이미지 개수 설정 (최대 16, 특징 맵 개수가 16보다 작으면 그 개수만큼)\n",
    "    images_per_row = min(n_features, 16)\n",
    "\n",
    "    # 열 개수 계산\n",
    "    n_cols = n_features // images_per_row\n",
    "    if n_features % images_per_row != 0:\n",
    "        n_cols += 1\n",
    "\n",
    "    # 이미지들을 쌓을 그리드 초기화\n",
    "    display_grid = np.zeros((height * n_cols, width * images_per_row), dtype=np.float32)\n",
    "\n",
    "    # 각 필터를 그리드에 채우기\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_index = col * images_per_row + row\n",
    "            if channel_index < n_features:\n",
    "                channel_image = activation[0, :, :, channel_index]\n",
    "                # 채널 정규화\n",
    "                channel_image -= channel_image.mean()\n",
    "                if channel_image.std() > 0: # 0으로 나누는 에러 방지\n",
    "                  channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                # 그리드에 채널 이미지 배치\n",
    "                display_grid[\n",
    "                    col * height : (col + 1) * height,\n",
    "                    row * width : (row + 1) * width,\n",
    "                ] = channel_image\n",
    "\n",
    "    # 그리드 스케일 조정\n",
    "    scale = 1.0 / max(width, height)\n",
    "    # OpenCV를 사용하여 특징 맵 출력\n",
    "    cv2.namedWindow(layer_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(layer_name, int(scale * display_grid.shape[1] * 20), int(scale * display_grid.shape[0] * 20)) # 가로 세로 비율 유지하면서 크기 조정\n",
    "    cv2.imshow(layer_name, display_grid)\n",
    "    while True:\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-0.9.13-py3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
